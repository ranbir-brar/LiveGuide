# Pipeline-level scheduling and safety state.
pipeline:
  # Expected YOLO processing frequency (frames per second).
  yolo_target_fps: 6.0

# LLM model and gating policy.
llm:
  # VLM backend provider: qwen_local | gemini_api
  provider: "qwen_local"

  # Hugging Face model ID used for first-time download.
  model_id: "Qwen/Qwen3-VL-2B-Instruct"
  # Local model directory (relative to project root).
  local_model_dir: "models/llm/weights/Qwen3-VL-2B-Instruct"

  # Gemini API settings (used when provider=gemini_api)
  gemini_model: "gemini-2.0-flash"
  gemini_api_key: ""
  gemini_api_key_env: "GEMINI_API_KEY"

  # Max generated tokens per LLM response.
  max_new_tokens: 80
  # Sampling temperature for generation.
  temperature: 0.2
  # Nucleus sampling top-p value.
  top_p: 0.9

  # Fixed VLM call interval in seconds.
  llm_call_interval_sec: 2.5
